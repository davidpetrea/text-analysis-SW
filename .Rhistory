group_by(character) %>%
unnest_tokens(word, dialogue) %>%
anti_join(stop_words)
#Frequencies
frequency<-bind_rows(mutate(tidy_ep4,episode="Ep4"),
mutate(tidy_ep5,episode="Ep5"),
mutate(tidy_ep6,episode="Ep6")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(episode,word) %>%
group_by(episode) %>%
mutate(proportion = n/sum(n)) %>%
select(-n) %>%
spread(episode,proportion) %>%
gather(episode, proportion, `Ep4`:`Ep5`)
View(frequency)
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
#Correlation tests
cor.test(data=frequency[frequency$episode=="Ep4",], ~proportion + `Ep6`)
cor.test(data=frequency[frequency$episode=="Ep5",], ~proportion + `Ep6`)
#There is a stronger correlation between episode 5 & 6
#Bigrams
trilogy_bigrams<-trilogy %>%
unnest_tokens(bigram, dialogue,token="ngrams",n=2)
trilogy_bigrams %>%
count(bigram,sort=TRUE)
bigrams_separated<-trilogy_bigrams %>%
separate(bigram,c("word1","word2"),sep=" ")
bigrams_filtered<-bigrams_separated %>%
filter(!word1 %in% en_stopwords) %>%
filter(!word2 %in% en_stopwords)
bigram_counts<-bigrams_filtered %>%
count(word1,word2,sort=TRUE)
bigram_counts
#Bigrams summary for trilogy using custom function
bigrams_summary(trilogy)
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
count(word1,word2,word3,sort=TRUE)
View(triology_trigrams)
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3)
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na()
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3)
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
count(word1,word2,word3,sort=TRUE)
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," ")) %>%
count(word1,word2,word3,sort=TRUE)
trigrams
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
trigrams_counts
}
trigrams_summary(trilogy)
View(trilogy)
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na()
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ")
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
count(word1,word2,word3,sort=TRUE)
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," ")) %>%
count(word1,word2,word3,sort=TRUE)
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(c(word1,word2,word3)," ")) %>%
count(word1,word2,word3,sort=TRUE)
trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word2,word3," ")) %>%
count(word1,word2,word3,sort=TRUE)
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word2,word3," ")) %>%
count(word1,word2,word3,sort=TRUE)
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords)
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word2,word3," "))
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," ")) %>%
count(trigram,sort=TRUE)
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams_summary(trilogy)
#tri-grams
triology_trigrams<-trilogy %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
print(trigrams)
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
print(trigrams_counts)
}
trigrams_summary(trilogy)
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
trigrams_top<-ggplot(data=trigrams_counts, aes(x=reorder(trigram, -n), y=n)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
print(trigrams_top)
}
trigrams_summary(trilogy)
trigrams_summary(ep4)
trigrams_summary(ep6)
trigrams_summary(ep5)
trigrams_summary(ep5)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(15)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(15)
triology_trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(1:15)
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(1:15)
trigrams_top<-ggplot(data=trigrams_counts, aes(x=reorder(trigram, -n), y=n)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
print(trigrams_top)
}
trigrams_summary(ep5)
trigrams_summary(6)
trigrams_summary(trilogy)
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(1:15)
trigrams_top<-ggplot(data=trigrams_counts, aes(x=reorder(trigram, -n), y=n)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Trigram", y="Frequency")
print(trigrams_top)
}
trigrams_summary(trilogy)
#Trigrams summary for trilogy using custom function
trigrams_summary(trilogy)
#Sections
trilogy_sections_words<-trilogy %>%
mutate(section=row_number()%/%10) %>%
filter(section>0) %>%
unnest_tokens(word, dialogue) %>%
filter(!word %in% en_stopwords)
View(trilogy_sections_words)
word_pairs <- triology_sections_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
word_pairs <- trilogy_sections_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
word_pairs %>%
filter(item1 == "yoda")
#Coef Phi
word_cors <- triology_sections_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
#Coef Phi
word_cors <- trilogy_sections_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors %>%
filter(item1 == "master")
word_cors %>%
filter(item1 == "yoda")
word_cors %>%
filter(item1 == "force")
#View correlations
word_cors %>%
filter(item1 %in% c("dark", "princess", "jedi", "master")) %>%
group_by(item1) %>%
top_n(6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = 'identity') +
facet_wrap(~item1, scales = 'free') +
coord_flip()
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
#NRC Lexicon
nrc_all<-get_sentiments("nrc")
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
nrc_fear<-get_sentiments("nrc") %>%
filter(sentiment =="fear")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
#Sections 25
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 10, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
View(trilogy_sentiments_10)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Afinn 20 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 20) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
#contribution to negative/positive sentiment
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#contribution to negative/positive sentiment
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
#Group by sentiment
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"),
max.words=100)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Sentiments and frequency associated with each word
sentiments <- tokens %>%
inner_join(nrc_all, "word") %>%
count(word, sentiment, sort=TRUE)
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc_all, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Tokens without stopwords
top_chars_tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
unnest_tokens(word, dialogue) %>%
anti_join(mystopwords, by="word")
# Most frequent words for each character
top_chars_tokens %>%
count(character, word) %>%
group_by(character) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=n)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
labs(x="Sentiment", y="Frequency") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Most relevant words for each character
top_chars_tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tf–idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
