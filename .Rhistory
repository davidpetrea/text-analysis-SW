low = "darkslategray4", high = "gray75") +
facet_wrap(~author,ncol=2) +
theme(legend.position="none") +
labs(y="Harry Houdini",x=NULL)
#Frequencies
frequency<-bind_rows(mutate(tidy_ep4,episode="Ep4"),
mutate(tidy_ep5,episode="Ep5"),
mutate(tidy_ep6,episode="Ep6")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(episode,word) %>%
group_by(episode) %>%
mutate(proportion = n/sum(n)) %>%
select(-n) %>%
spread(episode,proportion) %>%
gather(episode, proportion, `Ep4`:`Ep5`)
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
#Frequencies
frequency<-bind_rows(mutate(tidy_ep4,episode="Ep4"),
mutate(tidy_ep5,episode="Ep5"),
mutate(tidy_ep6,episode="Ep6")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(episode,word) %>%
group_by(episode) %>%
mutate(proportion = n/sum(n)) %>%
select(-n) %>%
spread(episode,proportion) %>%
gather(episode, proportion, `Ep4`:`Ep5`)
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
#Correlation tests
cor.test(data=frequency[frequency$episode=="Ep4",], ~proportion + `Ep6`)
cor.test(data=frequency[frequency$episode=="Ep5",], ~proportion + `Ep6`)
trilogy <- rbind(ep4, ep5, ep6)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
unnest_tokens(word, dialogue)
View(tokens)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
install.packages("reshape2")
ary(reshape2)
library(reshape2)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
#NRC Lexicon
nrc_all<-get_sentiments("nrc")
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
nrc_fear<-get_sentiments("nrc") %>%
filter(sentiment =="fear")
#Trust words for C-3PO
tidy_ep4 %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
#Trust words for C-3PO
tidy_ep4 %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_fear) %>%
count(word,set=TRUE)
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_fear) %>%
count(word,set=TRUE)
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
#Sections 50
trilogy_sentiments_50 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(character,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
unnest_tokens(word, dialogue)
View(tokens)
#tokenize
tidy_books <- books %>%
group_by(book) %>%
mutate(linenumber=row_number(),
chapter=cumsum(str_detect(text,regex("^CHAPTER [\\divxlc]", ignore_case=TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
View(tidy_books)
View(trilogy)
View(books)
View(ep4)
install.packages("gdata")
library(gdata)
trilogy<-combine(ep4, ep5, ep6)
View(trilogy)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source)
View(trilogy)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
str_remove(episode, 'ep')
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
str_remove(episode, "[ep]")
install.packages("stringr")
library(stringr)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV'))
View(trilogy)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV')) %>%
mutate(across('episode',str_replace, 'ep5', 'Episode V')) %>%
mutate(across('episode',str_replace, 'ep6', 'Episode VI'))
View(trilogy)
#tokenize
tidy_books <- books %>%
group_by(book) %>%
mutate(linenumber=row_number(),
chapter=cumsum(str_detect(text,regex("^CHAPTER [\\divxlc]", ignore_case=TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
View(tidy_books)
View(books)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV')) %>%
mutate(across('episode',str_replace, 'ep5', 'Episode V')) %>%
mutate(across('episode',str_replace, 'ep6', 'Episode VI'))
View(trilogy)
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV')) %>%
mutate(across('episode',str_replace, 'ep5', 'Episode V')) %>%
mutate(across('episode',str_replace, 'ep6', 'Episode VI'))
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number(),
dialogue=as.character(trilogy$dialogue)) %>%
unnest_tokens(word, dialogue)
View(trilogy)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
unnest_tokens(word, dialogue)
View(tokens)
#Combine all 3 episodes
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV')) %>%
mutate(across('episode',str_replace, 'ep5', 'Episode V')) %>%
mutate(across('episode',str_replace, 'ep6', 'Episode VI'))
View(trilogy)
View(books)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number(),
character=character) %>%
unnest_tokens(word, dialogue)
View(tokens)
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
ungroup()
unnest_tokens(word, dialogue)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
View(tokens)
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
#Sections 50
trilogy_sentiments_50 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(character,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
View(trilogy_sentiments_50)
ggplot(trilogy_sentiments_50, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
tidy_books %>%
filter(book=="Around the World in Eighty Days") %>%
inner_join(nrcjoy) %>%
count(word,set=TRUE)
#Sections 50
jules_sentiments_50 <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
View(jules_sentiments_50)
View(trilogy_sentiments_50)
#Sections 50
trilogy_sentiments_50 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
View(trilogy_sentiments_50)
View(jules_sentiments_50)
#Sections 25
trilogy_sentiments_25 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 25, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_25, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
View(trilogy_sentiments_25)
ggplot(trilogy_sentiments_25, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
ggplot(trilogy_sentiments_25, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Sections 25
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 10, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Afinn
around_world<-tidy_books %>%
filter(book=="Around the World in Eighty Days")
View(around_world)
#Sections 50/100
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
View(afinn_50)
#Graphs
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Afinn 50 -trilogy
afinn_50<-trilogy %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
View(tokens)
#Afinn 50 -tokens
afinn_50<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
View(afinn_50)
#Graphs
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Afinn 20 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 20) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
#Ordered
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Ordered
tidy_books %>%
anti_join(stop_words) %>%
count(word)
#Ordered
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Ordered
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Ordered
tokens %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud2(word,size=0.4))
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
View(wordcloud_test)
wordcloud2(wordcloud_test, size=0.4)
wordcloud2(wordcloud_test, size=0.4,figPath="./wordcloud_masks/yoda.png")
#Group by sentiment
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4,figPath="./wordcloud_masks/yoda.png")
View(wordcloud_test)
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4, figPath="./wordcloud_masks/r2d2.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/r2d2.png")
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/rebel alliance.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/yoda.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.5, figPath="../wordcloud_masks/vader.png")
#contribution to negative/positive sentiment
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.8, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.2, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.2, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.2, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.2, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.2, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4, figPath="./wordcloud_masks/vader.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
