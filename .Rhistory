acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=50)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("green20","red20"),
max.words=50)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=50)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE, random.color=FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE)) %>%
acast(word~sentiment,value.var="n",fill=0)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
#Order by freq
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(gutenbergr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
#install.packages('textdata')
#Around the world in 80 days, by Jules Verne
jules<-gutenberg_download(c(83,103,164),meta_fields ="title", mirror="http://mirrors.xmission.com/gutenberg/")
#rename title column -> book, remove ID column
books<-jules %>%
rename(book = title) %>%
select(-c(gutenberg_id))
#tokenize
tidy_books <- books %>%
group_by(book) %>%
mutate(linenumber=row_number(),
chapter=cumsum(str_detect(text,regex("^CHAPTER [\\divxlc]", ignore_case=TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
#NRC Lexicon
nrcjoy <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
tidy_books %>%
filter(book=="Around the World in Eighty Days") %>%
inner_join(nrcjoy) %>%
count(word,set=TRUE)
#Sections 50
jules_sentiments_50 <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(jules_sentiments_50, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
#Sections 100
jules_sentiments_100 <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book,index=linenumber %/% 100, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(jules_sentiments_100, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
#Afinn
around_world<-tidy_books %>%
filter(book=="Around the World in Eighty Days")
#Sections 50/100
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bind_rows(afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Wordclouds
library(wordcloud)
#Ordered
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Group by sentiment
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library('widyr')
library(janeaustenr)
library(stringr)
library(ggplot2)
library(igraph)
library(ggraph)
#Roman analizat: Mara de Ioan Slavici
book<-read.table('mara.txt', header = TRUE, sep = "\t", dec = ".")
load("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project/.RData")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
library('widyr')
library(stringr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(gdata)
library(stringr)
setwd("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project")
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4,figPath="./wordcloud_masks/yoda.png")
View(wordcloud_test)
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4,figPath="./wordcloud_masks/yoda.png")
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
#Group by sentiment
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
View(tokens)
View(tidy_hardy)
books<-gutenberg_download(c(4363,5827,4391,21995,1497,34901,4705),
meta_fields ="title",
mirror="http://mirrors.xmission.com/gutenberg/")
library(dplyr)
library(janeaustenr)
library(tidytext)
library(ggplot2)
library(gutenbergr)
books<-gutenberg_download(c(4363,5827,4391,21995,1497,34901,4705),
meta_fields ="title",
mirror="http://mirrors.xmission.com/gutenberg/")
#rename title column -> book, remove ID column
books<-books %>%
rename(book = title) %>%
select(-c(gutenberg_id))
View(books)
View(books)
View(books)
#tokenize & count
book_words <- books %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
View(book_words)
View(books)
# Transform the text to a tidy data structure with one token per row
episode_words <- trilogy %>%
unnest_tokens(word, dialogue) %>%
count(episode, word, sort = TRUE) %>%
ungroup()
View(episode_words)
total_words <- episode_words %>%
group_by(episode) %>%
summarize(total = sum(n))
View(total_words)
# Transform the text to a tidy data structure with one token per row
episode_words <- trilogy %>%
unnest_tokens(word, dialogue) %>%
count(episode, word, sort = TRUE) %>%
ungroup()
total_words <- episode_words %>%
group_by(episode) %>%
summarize(total = sum(n))
episode_words <- left_join(episode_words, total_words)
episode_words
#GGplot -
ggplot(episode_words, aes(n/total, fill = episode)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~episode, ncol = 2, scales = "free_y")
#Zipf's law
freq_by_rank <- episode_words %>%
group_by(episode) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 1000,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6618, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 50)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6618, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 50)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 30)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 20)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 15)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 800,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 900,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 700,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600700,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 12)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 15)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 5)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6618, slope = -1.1, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 5)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.700, slope = -1.036, color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 2)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 15)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 25)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 50)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 750,
rank > 15)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 15)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Linear model with a subset of ranks 10-1000
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6319, slope = -1.0635 , color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#bind_tf_idf
episode_words <- episode_words %>%
bind_tf_idf(word, episode, n)
episode_words
#Desc sorting by tf_idf & remove total column
episode_words %>%
select(-total) %>%
arrange(desc(tf_idf))
episode_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(episode) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = episode)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~episode, ncol = 2, scales = "free") +
coord_flip()
#custom stop words
mystopwords <- data_frame(word = c("na"))
episode_words <- anti_join(episode_words, mystopwords, by = "word")
episode_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(episode) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = episode)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~episode, ncol = 2, scales = "free") +
coord_flip()
