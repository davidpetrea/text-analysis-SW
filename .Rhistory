count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library('widyr')
library(janeaustenr)
library(stringr)
library(ggplot2)
library(igraph)
library(ggraph)
#Roman analizat: Mara de Ioan Slavici
book<-read.table('mara.txt', header = TRUE, sep = "\t", dec = ".")
#NOTE: Run this script first so all required libraries are
#loaded and data is formatted properly for analysis.
library(dplyr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
library('widyr')
library(stringr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(gdata)
library(stringr)
library(stopwords)
setwd("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project")
#setwd("C:/Users/wyver/Desktop/Master/An 2/Analiza text/project")
# Read the text files
ep4 <- read.table("./texts/SW_EpisodeIV.txt")
ep5 <- read.table("./texts/SW_EpisodeV.txt")
ep6 <- read.table("./texts/SW_EpisodeVI.txt")
#Combine all 3 episodes
trilogy<-combine(ep4, ep5, ep6) %>%
rename(episode=source) %>%
mutate(across('episode',str_replace, 'ep4', 'Episode IV')) %>%
mutate(across('episode',str_replace, 'ep5', 'Episode V')) %>%
mutate(across('episode',str_replace, 'ep6', 'Episode VI'))
#Stop words
en_stopwords<-stopwords::stopwords("en", source = "snowball")
mystopwords <- data.frame(word=en_stopwords)
load("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project/.RData")
View(tidy_ep4)
#Tokenize each episode
tidy_ep4 <-ep4 %>%
group_by(character) %>%
unnest_tokens(word, dialogue) %>%
anti_join(stop_words)
tidy_ep5 <-ep5 %>%
group_by(character) %>%
unnest_tokens(word, dialogue) %>%
anti_join(stop_words)
tidy_ep6 <-ep6 %>%
group_by(character) %>%
unnest_tokens(word, dialogue) %>%
anti_join(stop_words)
#Frequencies
frequency<-bind_rows(mutate(tidy_ep4,episode="Ep4"),
mutate(tidy_ep5,episode="Ep5"),
mutate(tidy_ep6,episode="Ep6")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(episode,word) %>%
group_by(episode) %>%
mutate(proportion = n/sum(n)) %>%
select(-n) %>%
spread(episode,proportion) %>%
gather(episode, proportion, `Ep4`:`Ep5`)
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
#Correlation tests
cor.test(data=frequency[frequency$episode=="Ep4",], ~proportion + `Ep6`)
cor.test(data=frequency[frequency$episode=="Ep5",], ~proportion + `Ep6`)
#There is a stronger correlation between episode 5 & 6
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
#Sections 10
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 10, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Afinn 20 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 20) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
#contribution to negative/positive sentiment
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc_all, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Tokens without stopwords
top_chars_tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
unnest_tokens(word, dialogue) %>%
anti_join(mystopwords, by="word")
# Most frequent words for each character - before tf_idf
top_chars_tokens %>%
count(character, word) %>%
group_by(character) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=n)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
labs(x="Sentiment", y="Frequency") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Most relevant words for each character - after tf_idf
top_chars_tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tfâ€“idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
episode_words <- trilogy %>%
unnest_tokens(word, dialogue) %>%
count(episode, word, sort = TRUE) %>%
ungroup()
total_words <- episode_words %>%
group_by(episode) %>%
summarize(total = sum(n))
episode_words <- left_join(episode_words, total_words)
#Distributions
ggplot(episode_words, aes(n/total, fill = episode)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~episode, ncol = 2, scales = "free_y")
#Zipf's law
freq_by_rank <- episode_words %>%
group_by(episode) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
#Plot frequencies by rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-600
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Plot linear model over freq using estimated intercept and slope from above
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6319, slope = -1.0635 , color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#bind_tf_idf
episode_words <- episode_words %>%
bind_tf_idf(word, episode, n)
episode_words
#Desc sorting by tf_idf & remove total column
episode_words %>%
select(-total) %>%
arrange(desc(tf_idf))
#Plot highest tf_idf words for each episode
episode_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(episode) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = episode)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~episode, ncol = 2, scales = "free") +
coord_flip()
#Bigrams summary for trilogy using custom function
bigrams_summary(trilogy)
text_summary(ep4)
script_summary <-function(text) {
print("Dialogues count:")
print(length(text$dialogue)) #Replace dialogue with name of column where the text is
print("Characters count:")
print(length(unique(text$character)))   #Expects a column containing the character saying the line
}
top_characters_summary <-function(text) {
top_characters <- as.data.frame(sort(table(text$character), decreasing=TRUE))[1:10,]
# Visualization
top_graph <-ggplot(data=top_characters, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
print(top_graph)
}
text_summary(ep4)
top_characters_summary(ep4)
top_characters_summary(ep5)
top_characters_summary(ep6)
#Bigrams summary for trilogy using custom function
bigrams_summary(trilogy)
#Trigrams summary for trilogy using custom function
trigrams_summary(trilogy)
#Sections
trilogy_sections_words<-trilogy %>%
mutate(section=row_number()%/%10) %>%
filter(section>0) %>%
unnest_tokens(word, dialogue) %>%
filter(!word %in% en_stopwords)
word_pairs <- trilogy_sections_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
word_pairs %>%
filter(item1 == "yoda")
word_pairs %>%
filter(item1 == "dark")
#Coef Phi
word_cors <- trilogy_sections_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors %>%
filter(item1 == "force")
#View correlations
word_cors %>%
filter(item1 %in% c("dark", "princess", "jedi", "master")) %>%
group_by(item1) %>%
top_n(6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = 'identity') +
facet_wrap(~item1, scales = 'free') +
coord_flip()
set.seed(123)
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 4) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 6) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() %>%
par(bg="aliceblue")
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
par(bg="aliceblue")
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE,color='red') +
geom_node_point(color = "orange", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE,color='red') +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
script_summary(ep4)
script_summary(ep5)
script_summary(ep6)
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
#Correlation tests
cor.test(data=frequency[frequency$episode=="Ep4",], ~proportion + `Ep6`)
cor.test(data=frequency[frequency$episode=="Ep5",], ~proportion + `Ep6`)
tokens <- trilogy %>%
group_by(episode) %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
#Trust words for C-3PO
tokens %>%
filter(character=="VADER") %>%
inner_join(nrc_fear) %>%
count(word,set=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE,sort=TRUE)
nrc_fear<-get_sentiments("nrc") %>%
filter(sentiment =="fear")
#Fear words for Vader
tokens %>%
filter(character=="VADER") %>%
inner_join(nrc_fear) %>%
count(word,set=TRUE,sort=TRUE)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
nrc_fear<-get_sentiments("nrc") %>%
filter(sentiment =="fear")
#Fear words for Vader
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,set=TRUE,sort=TRUE)
