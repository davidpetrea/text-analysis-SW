group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bind_rows(afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Wordclouds
library(wordcloud)
#Ordered
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Group by sentiment
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library('widyr')
library(janeaustenr)
library(stringr)
library(ggplot2)
library(igraph)
library(ggraph)
#Roman analizat: Mara de Ioan Slavici
book<-read.table('mara.txt', header = TRUE, sep = "\t", dec = ".")
load("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project/.RData")
View(afinn_20)
View(afinn_20)
load("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project/.RData")
#loaded and data is formatted properly for analysis.
library(dplyr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
library('widyr')
library(stringr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(gdata)
library(stringr)
library(stopwords)
setwd("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project")
#setwd("C:/Users/wyver/Desktop/Master/An 2/Analiza text/project")
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,set=TRUE,sort=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,sort=TRUE)
nrc_fear<-get_sentiments("nrc") %>%
filter(sentiment =="fear")
#Fear words for Vader
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
nrc_trust <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
#Trust words for C-3PO
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,sort=TRUE)
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
tokens %>%
filter(character=="VADER") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
#Fear words for Luke
tokens %>%
filter(character=="THREEPIO") %>%
inner_join(nrc_trust) %>%
count(word,sort=TRUE)
#Trust words for C-3PO
tokens %>%
filter(character=="LUKE") %>%
inner_join(nrc_fear) %>%
count(word,sort=TRUE)
#Fear words for Luke
#Sections 50
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Sections 50
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 20, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
#Sections 10
trilogy_sentiments_10 <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(episode,index=linenumber %/% 10, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
View(trilogy_sentiments_10)
#Afinn 20 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 20) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Afinn 10 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 10) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Afinn 20 -tokens
afinn_20<-tokens %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 20) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_20) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
#contribution to negative/positive sentiment
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Wordclouds
#Ordered
wordcloud_test<-tokens %>%
anti_join(stop_words) %>%
count(word)
wordcloud2(wordcloud_test, size=0.4)
# Sentiments and frequency associated with each word
sentiments <- tokens %>%
inner_join(nrc_all, "word") %>%
count(word, sentiment, sort=TRUE)
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc_all, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Tokens without stopwords
top_chars_tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
unnest_tokens(word, dialogue) %>%
anti_join(mystopwords, by="word")
# Most relevant words for each character - after tf_idf
top_chars_tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tfâ€“idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Most relevant words for each character - tf_idf
top_chars_tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Transform the text to a tidy data structure with one token per row
episode_words <- trilogy %>%
unnest_tokens(word, dialogue) %>%
count(episode, word, sort = TRUE) %>%
ungroup()
total_words <- episode_words %>%
group_by(episode) %>%
summarize(total = sum(n))
episode_words <- left_join(episode_words, total_words)
#Distributions
ggplot(episode_words, aes(n/total, fill = episode)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~episode, ncol = 2, scales = "free_y")
#Zipf's law
freq_by_rank <- episode_words %>%
group_by(episode) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
#Zipf's law
freq_by_rank <- episode_words %>%
group_by(episode) %>%
mutate(rank = row_number(),
`term frequency` = n/total)
freq_by_rank
#Plot frequencies by rank
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#Linear model with a subset of ranks 10-600
rank_subset <- freq_by_rank %>%
filter(rank < 600,
rank > 10)
lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
#Plot linear model over freq using estimated intercept and slope from above
freq_by_rank %>%
ggplot(aes(rank, `term frequency`, color = episode)) +
geom_abline(intercept = -0.6319, slope = -1.0635 , color = "gray50", linetype = 2) +
geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
scale_x_log10() +
scale_y_log10()
#bind_tf_idf
episode_words <- episode_words %>%
bind_tf_idf(word, episode, n)
episode_words
#Desc sorting by tf_idf & remove total column
episode_words %>%
select(-total) %>%
arrange(desc(tf_idf))
#Plot highest tf_idf words for each episode
episode_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(episode) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = episode)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~episode, ncol = 2, scales = "free") +
coord_flip()
#Bigrams summary for trilogy using custom function
bigrams_summary(trilogy)
#Trigrams summary for trilogy using custom function
trigrams_summary(trilogy)
#Sections
trilogy_sections_words<-trilogy %>%
mutate(section=row_number()%/%10) %>%
filter(section>0) %>%
unnest_tokens(word, dialogue) %>%
filter(!word %in% en_stopwords)
word_pairs <- trilogy_sections_words %>%
pairwise_count(word, section, sort = TRUE)
word_pairs
word_pairs %>%
filter(item1 == "yoda")
#Coef Phi
word_cors <- trilogy_sections_words %>%
group_by(word) %>%
filter(n() >= 20) %>%
pairwise_cor(word, section, sort = TRUE)
word_cors
word_cors %>%
filter(item1 == "force")
#View correlations
word_cors %>%
filter(item1 %in% c("dark", "princess", "jedi", "master")) %>%
group_by(item1) %>%
top_n(6) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = 'identity') +
facet_wrap(~item1, scales = 'free') +
coord_flip()
set.seed(123)
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
custom_wordcloud(ep4)
custom_wordcloud <-function(text) {
# Transform the text to a tidy data structure with one token per row
episode_tokens <- text %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
#Wordcloud
wordcloud_tokens <-episode_tokens %>%
anti_join(stop_words) %>%
count(word)
print(wordcloud2(wordcloud_tokens, size=0.7))
}
custom_wordcloud(ep4)
custom_wordcloud <-function(text) {
# Transform the text to a tidy data structure with one token per row
episode_tokens <- text %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
#Wordcloud
wordcloud_tokens <-episode_tokens %>%
anti_join(stop_words) %>%
count(word)
print(wordcloud2(wordcloud_tokens, size=0.7,backgroundColor="black"))
}
custom_wordcloud(ep4)
custom_wordcloud(ep5)
custom_wordcloud(ep6)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
par(bg = 'blue')
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
par(bg = 'black')
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
par(bg = 'black')
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"),title.colors = "#F5A207", max.words=100)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"),title.colors = "#F5A207", max.words=100)
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"),match.colors = TRUE, max.words=100)
par(bg = 'black')
# Positive and negative words -bing lexicon
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"),match.colors = TRUE, max.words=100)
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
par(bg = 'black')
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=2,scales="free_x")
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=1,scales="free_x")
par(bg = 'black')
ggplot(trilogy_sentiments_10, aes(index,sentiment,fill=episode)) +
geom_col(show.legend = FALSE) +
facet_wrap(~episode,ncol=1,scales="free_x")
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
par(bg = 'blue')
#nodes
word_cors %>%
filter(correlation > .15) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
geom_node_point(color = "orange", size = 3) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void()
