inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(score)) %>%
mutate(method="AFINN")
around_world<-tidy_books %>%
filter(book=="Around the World in Eighty Days")
afinn<-around_world %>%
inner_join(get_sentiments("afinn"))
afinn<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50)
afinn<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
View(afinn)
bind_rows(afinn) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=1,scales="free_y")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
bind_rows(afinn_50,afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=1,scales="free_y")
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
bind_rows(afinn_50,afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=1,scales="free_y")
View(afinn_100)
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
bind_rows(afinn_50,afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=1,scales="free_y")
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
bind_rows(afinn_50,afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=1,scales="free_y")
bind_rows(afinn_50,afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bind_rows(afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bing_word_counts <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
# top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(250) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
library(wordcloud)
#Wordclouds
library(wordcloud)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=50))
library(reshape2)
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=50)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("green20","red20"),
max.words=50)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=50)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE, random.color=FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE)) %>%
acast(word~sentiment,value.var="n",fill=0)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
#Order by freq
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(gutenbergr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
#install.packages('textdata')
#Around the world in 80 days, by Jules Verne
jules<-gutenberg_download(c(83,103,164),meta_fields ="title", mirror="http://mirrors.xmission.com/gutenberg/")
#rename title column -> book, remove ID column
books<-jules %>%
rename(book = title) %>%
select(-c(gutenberg_id))
#tokenize
tidy_books <- books %>%
group_by(book) %>%
mutate(linenumber=row_number(),
chapter=cumsum(str_detect(text,regex("^CHAPTER [\\divxlc]", ignore_case=TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
#NRC Lexicon
nrcjoy <-get_sentiments("nrc") %>%
filter(sentiment =="trust")
tidy_books %>%
filter(book=="Around the World in Eighty Days") %>%
inner_join(nrcjoy) %>%
count(word,set=TRUE)
#Sections 50
jules_sentiments_50 <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book,index=linenumber %/% 50, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(jules_sentiments_50, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
#Sections 100
jules_sentiments_100 <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book,index=linenumber %/% 100, sentiment) %>%
spread(sentiment,n,fill=0) %>%
mutate(sentiment = positive - negative)
ggplot(jules_sentiments_100, aes(index,sentiment,fill=book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol=2,scales="free_x")
#Afinn
around_world<-tidy_books %>%
filter(book=="Around the World in Eighty Days")
#Sections 50/100
afinn_50<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 50) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
afinn_100<-around_world %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index=linenumber %/% 100) %>%
summarise(sentiment=sum(value)) %>%
mutate(method="AFINN")
#Graphs
bind_rows(afinn_50) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
bind_rows(afinn_100) %>%
ggplot(aes(index,sentiment,fill=method)) +
geom_col(show.legend=FALSE) +
facet_wrap(~method,ncol=2,scales="free_y")
#Word freq contribution
bing_word_counts <-tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
top_n(5) %>%
ungroup() %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n,fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment,scales="free_y") +
labs(y="Contribution to sentiment",
x=NULL) + coord_flip()
#Wordclouds
library(wordcloud)
#Ordered
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words=100,random.order = FALSE))
#Group by sentiment
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word,sentiment,sort=TRUE) %>%
acast(word~sentiment,value.var="n",fill=0) %>%
comparison.cloud(colors=c("gray20","gray80"),
max.words=100)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library('widyr')
library(janeaustenr)
library(stringr)
library(ggplot2)
library(igraph)
library(ggraph)
#Roman analizat: Mara de Ioan Slavici
book<-read.table('mara.txt', header = TRUE, sep = "\t", dec = ".")
load("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project/.RData")
text_summary(ep4)
#NOTE: Run this script first so all required libraries are
#loaded and data is formatted properly for analysis.
library(dplyr)
library(tidyverse)
library(tidytext)
library(tidyr)
library(scales)
library(ggplot2)
library('widyr')
library(stringr)
library(igraph)
library(ggraph)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(gdata)
library(stringr)
library(stopwords)
setwd("C:/Users/wyver/Desktop/Master/An 2/Sem 1/Text/project")
text_summary(ep4)
#Custom function that prints a general summary (bigrams, wordcloud, top characters) given a text data frame
#with 2 columns: character and dialogue
text_summary <- function(text) {
print("Dialogues count:")
print(length(text$dialogue)) #Replace dialogue with name of column where the text is
print("Characters count:")
print(length(unique(text$character)))   #Expects a column containing the character saying the line
top_characters <- as.data.frame(sort(table(text$character), decreasing=TRUE))[1:10,]
# Visualization
top_graph <-ggplot(data=top_characters, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
print(top_graph)
#Wordcloud
custom_wordcloud(text)
#Bigrams
bigrams_summary(text)
#Trigrams
trigrams_summary(text)
}
bigrams_summary <- function(text) {
bigrams<-text %>%
unnest_tokens(bigram, dialogue,token="ngrams",n=2) %>%
separate(bigram,c("word1","word2"),sep=" ") %>%
filter(!word1 %in% en_stopwords) %>%
filter(!word2 %in% en_stopwords) %>%
mutate(bigram = paste(word1,word2," ")) %>%
drop_na()
bigram_counts <-bigrams %>%
count(bigram,sort=TRUE) %>%
top_n(15) %>%
slice(1:15)
bigram_counts
bigrams_top<-ggplot(data=bigram_counts, aes(x=reorder(bigram, -n), y=n)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
print(bigrams_top)
}
trigrams_summary<-function(text) {
trigrams<-text %>%
unnest_tokens(trigram, dialogue,token="ngrams",n=3) %>%
drop_na() %>%
separate(trigram,c("word1","word2","word3"),sep=" ") %>%
filter(!word1 %in% en_stopwords,
!word2 %in% en_stopwords,
!word3 %in% en_stopwords) %>%
mutate(trigram = paste(word1,word2,word3," "))
trigrams_counts <-trigrams %>%
count(trigram,sort=TRUE) %>%
top_n(15) %>%
slice(1:15)
trigrams_top<-ggplot(data=trigrams_counts, aes(x=reorder(trigram, -n), y=n)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Trigram", y="Frequency")
print(trigrams_top)
}
custom_wordcloud <-function(text) {
# Transform the text to a tidy data structure with one token per row
episode_tokens <- text %>%
mutate(linenumber=row_number()) %>%
ungroup() %>%
unnest_tokens(word, dialogue)
#Wordcloud
wordcloud_tokens <-episode_tokens %>%
anti_join(stop_words) %>%
count(word)
print(wordcloud2(wordcloud_tokens, size=0.4))
}
text_summary(ep4)
#Plot highest tf_idf words for each episode
episode_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(episode) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = episode)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~episode, ncol = 2, scales = "free") +
coord_flip()
# Most relevant words for each character - after tf_idf
top_chars_tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tf–idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc_all, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
#Scales
ggplot(frequency, aes(x = proportion, y = `Ep6`,
color = abs(`Ep6`- proportion))) +
geom_abline(color="gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~episode,ncol=2) +
theme(legend.position="none") +
labs(y="Ep6",x=NULL)
